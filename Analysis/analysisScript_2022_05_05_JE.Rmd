---
title: "R Notebook"
output:
  html_document:
    toc: true
    theme: united
---

```{r}
library(ggplot2)
library(lme4)
library(lmerTest)
library(effects)
library(ggeffects)
library(corrr)
library(dplyr)
library(MuMIn)
library(r2glmm)
library(multicon)
library(RVAideMemoire)
library(effsize)
library(ez)
library(psychReport)
library(jtools)
library(compiler)
library(ordinal)
Cconfint<-cmpfun(confint)
```
# Set dir for figures

```{r}
curWd <- "~/Documents/UC Riverside/Studies/fMRI Feedback/Manuscript/"
```


```{r}
setwd("/Volumes/GoogleDrive/My Drive/Volumes/Research Project/Social Evaluative Feedback fMRI/Behavioral Data Analysis/output/")
AsymLR.PlotDf <- read.csv("PlotDf.csv")
AsymLR.ParamDf <- read.csv("GraphMix.ParamDf.csv")
AIClist <- read.csv("AIClist.csv")
posLR.corDf <- read.csv("posLR.corDf.csv")
negLR.corDf <- read.csv("negLR.corDf.csv")
mix.corDf <- read.csv("mix.corDf.csv")
fullTDdf <- read.csv("fullTDdf.csv")
tBt_Df <- read.csv("trialBytrial.Df.csv")
summaryAgChange <- read.csv("summaryAgChange.csv")
summaryAgVChange <- read.csv("summaryAgVChange.csv")
AsymLR.corDf <- read.csv("AsymLR.corDf.csv")
agDf <- read.csv("agDf.csv")
agVDf <- read.csv("agVDf.csv")
fullTDdf$valFeed <- as.factor(fullTDdf$valFeed)
fullTDdf$clustType <- as.factor(fullTDdf$clustType)
```

# Tests of Learning Rates

## Permutation T-Test and Wilcoxon Signed Rank Test, Means and Medians

```{r}
permTest<-wPerm::perm.paired.loc(AsymLR.ParamDf$posLR, AsymLR.ParamDf$negLR, mean,
                alternative = c("two.sided"),
                R = 100000)

describe(AsymLR.PlotDf$posLR)

describe(AsymLR.PlotDf$negLR)

permTest
mean(permTest$Perm.values)
```

# Test of Mixture Parameter

## Wilcoxon Signed

```{r}
res <- wilcox.test(AsymLR.ParamDf$mix, mu = .50)
res

mean(AsymLR.ParamDf$mix)
sd(AsymLR.ParamDf$mix)

perm1samp <- function(x,myfun=mean,mu=0,nsamp=10000,
alternative=c("two.sided","less","greater")){
x = x - mu
n = length(x)
theta.hat = myfun(x)
gmat = replicate(nsamp,sample(x=c(1,-1),size=n,replace=TRUE))
theta.mc = apply(gmat*abs(x),2,myfun)
if(alternative[1]=="less"){
aslperm = sum(theta.mc <= theta.hat) / nsamp
} else if(alternative[1]=="greater"){
aslperm = sum(theta.mc >= theta.hat) / nsamp
} else{
aslperm = sum(abs(theta.mc) >= abs(theta.hat)) / nsamp
}
list(theta.hat=theta.hat,theta.mc=theta.mc,asl=aslperm)
}

out<-perm1samp(x=AsymLR.ParamDf$mix, mu=.50)
out$asl
```

## Scatterplot

```{r}
trsup <- data.frame(x=c(-2,-2,2), y=c(-2,2,2))
trinf <- data.frame(x=c(-2,2,2), y=c(-2,-2,2))

df <- data.frame(x = sample(1:100, 100, replace = FALSE), y = sample(1:100, 100, replace=FALSE))

p1 <- ggplot(AsymLR.ParamDf, aes(x=negLR, y=posLR)) + geom_point() + xlab("Negative Learning Rates") + ylab("Positive Learning Rates") + geom_abline(intercept = 0, slope = 1)+ theme(axis.text=element_text(size=9),
        axis.title=element_text(size=9,face="bold")) + theme(legend.text = element_text(size=9)) + theme(panel.border = element_rect(colour = "black", fill = NA, size =1)) + theme(legend.title = element_blank()) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) + geom_polygon(aes(x=x,y=y), data = trsup, fill= "dodgerblue3", alpha = .4) + geom_polygon(aes(x=x,y=y), data = trinf, fill = "#FF000066", alpha = .4) + coord_cartesian(xlim=c(0,1), ylim=c(0,1))
p1


ggsave(paste0(curWd,"Figures/probLineP.tiff"), dpi=600)
```

# MLMs of Value Estimates Predicting Trial-by-Trial Self-Evaluations

Value estimates moderately predict trial-by-trial self-evaluations, but moreso when interacting with trials. Oddly, at later trials, value estimates are less predictive of self-evaluations.

```{r}
# Null model
m1 <- lmer(scale(selfRespT1) ~ 1 + ( 1 | subID ), data = tBt_Df, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))

# SE as Random Intercept
m2 <- lmer(scale(selfRespT1) ~ scale(valEst) + ( 1 | subID ), data = tBt_Df, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))

# LRT
anova(m2, m1)
# Rsq diff
r.squaredGLMM(m2) - r.squaredGLMM(m1)

# SE as random slope for subject
m3 <- lmer(scale(selfRespT1) ~ scale(valEst) + ( scale(valEst) | subID ), data = tBt_Df, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
summary(m3)

# LRT
anova(m3, m2)
# Rsq diff
r.squaredGLMM(m3) - r.squaredGLMM(m2)

# Trait as random factor
m4 <- lmer(scale(selfRespT1) ~ scale(valEst) + ( scale(valEst) | subID ) + (scale(valEst) | Trait ), data = tBt_Df, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
summary(m4)

# LRT
anova(m4, m3)
# rsqdiff
r.squaredGLMM(m4) - r.squaredGLMM(m3)


m5 <- lmer(scale(selfRespT1) ~ scale(valEst) + scale(ent) + ( scale(valEst) + scale(ent) | subID ) + (scale(valEst) + scale(ent) | Trait ), data = tBt_Df, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
summary(m5)

anova(m5, m4)
# rsqdiff
r.squaredGLMM(m5) - r.squaredGLMM(m4)
r2beta(m5)
```


```{r}
tab_model(m4, show.stat=T, show.r2 = T, show.se = T, pred.labels = c("Intercept", "Expectation"), string.pred = c("Fixed Effects"), string.est = "Coef.", string.se = "SE", string.stat = "t", digits = 3, digits.re = 3, emph.p = F, dv.labels = "Learning Model", file = paste0(curWd,"Tables/LearningTable.doc") )
```

## Ordinal Logistic

Testing as ordinal logistic

```{r}
tBt_Df$selfRespT1F <- as.factor(tBt_Df$selfRespT1)
OR.TRT <- clmm(selfRespT1F ~ scale(valEst)  + ( scale(valEst) | subID ) + (scale(valEst) | Trait ), data = tBt_Df)
tab_model(OR.TRT)
```

# MLMs of Value Estimates Predicting Re-Evaluations

```{r}
# dataset with NAs removed
rDf <- na.omit(fullTDdf)
# Null Model 
m0 <- lmer(scale(selfRespT2) ~ 1 + (1 |subID) + (1 | Trait), data = rDf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)) )


# Base Residualized Change Model (T1 predicting T2)
m1.2 <- lmer(scale(as.numeric(selfRespT2)) ~ scale(selfRespT1) + (1 |subID) + (1 | Trait), data = rDf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)) )
# LRT
anova(m1.2, m0)
# rsq diff
r.squaredGLMM(m1.2)[1] - r.squaredGLMM(m0)[1]
m1 <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + (1 |subID) + (1 | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))

# Prediction Error Main Effect Model
m2F <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(predError) + ( scale(predError) |subID) + ( scale(predError) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m2F, m1)
# rsq diff
r.squaredGLMM(m2F)[1] - r.squaredGLMM(m1)[1]

# Social Expectation (Similarity) Main Effect Model
m2Vs <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstS) + ( scale(valEstS) |subID) + ( 1 | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))

# LRT
anova(m2Vs, m1)
# rsq diff
r.squaredGLMM(m2Vs) - r.squaredGLMM(m1)

# Social Expectation (Error) Main Effect Model
m2Vp <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstPE) + ( scale(valEstPE) |subID) + ( 1 | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))

# LRT
anova(m2Vp, m1)
# rsq diff
r.squaredGLMM(m2Vp) - r.squaredGLMM(m1)

# Social Expectation (Combined) Main Effect Model
m2Vc <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstF) + ( scale(valEstF) |subID) + (1 | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))

# LRT
anova(m2Vc, m1)
# rsq diff
r.squaredGLMM(m2Vc) - r.squaredGLMM(m1)

# Prediction Error and Social Expectation Model
m2 <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstF) + scale(predError) + (  scale(valEstF) + scale(predError) |subID) + (scale(predError) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m2, m2F)
# rsq diff
r.squaredGLMM(m2)[1] - r.squaredGLMM(m2F)[1]

# Prediction Error, Social Expectation, and Indegree and Outdegree Centrality Model
m2IO <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstF) + scale(predError) + scale(inDegree) + scale(outDegree) + (  scale(valEstF) + scale(predError) |subID) + (scale(predError) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m2IO, m2)
# rsq diff
r.squaredGLMM(m2IO)[1] - r.squaredGLMM(m2)[1]

# Prediction Error Interacting with Outdegree Model
m3FO <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstF) + scale(predError) + scale(inDegree) + scale(predError)*scale(outDegree) + (  scale(valEstF) + scale(predError) |subID) + (scale(predError) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m2IO, m3FO)
# rsq diff
r.squaredGLMM(m3FO)[1] - r.squaredGLMM(m2IO)[1]

# Prediction Error Interacting with Indegree Model
m3FI <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstF) + scale(predError) + scale(outDegree) + scale(predError)*scale(inDegree) + (  scale(valEstF) + scale(predError) |subID) + (scale(predError) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m2IO, m3FI)
# rsq diff
r.squaredGLMM(m3FI)[1] - r.squaredGLMM(m2IO)[1]

# Social Expectation Interacting with Outdegree Model
m3VO <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstF) + scale(predError) + scale(inDegree) + scale(valEstF)*scale(outDegree) + (  scale(valEstF) + scale(predError) |subID) + (scale(predError)1 | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m2IO, m3VO)
# rsq diff
r.squaredGLMM(m3VO)[1] - r.squaredGLMM(m2IO)[1]

# Social Expectation Interacting with Indegree Model
m3VI <- lmer(scale(selfRespT2) ~ scale(selfRespT1) + scale(valEstF) + scale(predError) + scale(outDegree) + scale(valEstF)*scale(inDegree) + (  scale(valEstF) + scale(predError) |subID) + (scale(predError) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m2IO, m3VI)
# rsq diff
r.squaredGLMM(m3VI) - r.squaredGLMM(m2IO)

# Introducing random slopes for both prediction error and social expectation
ZreEvalModelFinal <- lmer(scale(as.numeric(selfRespT2)) ~ scale(selfRespT1) + scale(valEstF) + scale(predError)*scale(outDegree) + scale(valEstF)*scale(outDegree) + ( scale(predError) + scale(valEstF) |subID) + (scale(predError) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
# LRT
anova(m3,ZreEvalModelB)
# rsq diff
r.squaredGLMM(ZreEvalModelB) - r.squaredGLMM(m3)
summary(ZreEvalModelFinal)
# all effect sizes
b<-r2beta(ZreEvalModelFinal)
data.frame(b$Effect, b$Rsq)
# confidence intervals
confint(ZreEvalModelFinal)
```

```{r}
finalRE <- lmer(scale(as.numeric(selfRespT2)) ~ scale(selfRespT1) + scale(valEstF) + scale(predError)*scale(outDegree) + scale(valEstF)+scale(outDegree) + ( scale(predError) + scale(valEstF) |subID) + (scale(predError) + scale(valEstF) | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
tab_model(finalRE)
tab_model(finalRE, show.stat=T, show.r2 = T, show.se = T, pred.labels = c("Intercept", "T1 Evaluation", "Expectation", "PE", "Outdegree", "PE * Outdegree"), string.pred = c("Fixed Effects"), string.est = "Coef.", string.se = "SE", string.stat = "t", digits = 3, digits.re = 3, emph.p = F, dv.labels = "Change Model", file = paste0(curWd,"Tables/ChangeTable.doc") )
```


## As ordinal logistic

```{r}
fullTDdf$selfRespT2F <- as.factor(fullTDdf$selfRespT2)
fullTDdf$changeScoreZ <- scale(fullTDdf$changeScore)
fullTDdf$selfRespT1z <- scale(fullTDdf$selfRespT1)
fullTDdf$valEstFz <- scale(fullTDdf$valEstF)
fullTDdf$predErrorz <- scale(fullTDdf$predError)
fullTDdf$outDegreez <- scale(fullTDdf$outDegree)
fullTDdf$changeScoreF <- as.factor(as.numeric(fullTDdf$changeScore)+7)

RE.ORD <- clmm( selfRespT2 ~ selfRespT1z + valEstFz + predErrorz*outDegreez + valEstFz + ( predErrorz + valEstFz |subID) + (1 | Trait), data = fullTDdf)
summary(RE.ORD)
```

## predError by Outdegree Interaction Plot

```{r}
changePlotModel <- lmer( changeScore ~ valEstF + predError*outDegree + inDegree + ( valEstF + predError |subID) + ( 1 |Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))

T2T1m <- lmer( selfRespT2 ~ selfRespT1 + ( selfRespT1 |subID) + ( selfRespT1 | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
T2T1res <- resid(T2T1m)
fullTDdf$residuals <- NA
fullTDdf$residuals[!is.na(fullTDdf$selfRespT1) & !is.na(fullTDdf$selfRespT2)] <- T2T1res

T2T1m <- lmer( residuals ~ valEstF + predError*outDegree + ( valEstF + predError |subID) + ( valEstF + predError | Trait), data = fullTDdf, 
              control = lmerControl(optimizer = "bobyqa",
                                    optCtrl = list(maxfun=2e5)))
```


```{r}
p <- ggpredict(T2T1m, c("outDegree","predError"))
plotOUTPE<-ggplot(p, aes(x, predicted)) +  geom_line(aes(linetype=group, color=group)) + geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=group), alpha=0.15) + scale_linetype_discrete(labels = c("Low PE","Mid PE", "High PE")) + scale_color_discrete(labels = c("Low PE","Mid PE", "High PE")) + scale_fill_discrete( 
                      labels=c("Low PE", "Mid PE", "High PE")) + theme(
    legend.position = c(.6, .065),
    legend.justification = c("left", "bottom"),
    legend.box.just = "left",
    legend.margin = margin(6, 6, 6, 6)
    ) + theme(axis.text=element_text(size=12),
        axis.title=element_text(size=12,face="bold")) + theme(legend.text = element_text(size=12)) + theme(panel.border = element_rect(colour = "black", fill = NA, size =1)) + theme(legend.title = element_blank()) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  xlab("Outdegree Centrality") + ylab("Residuals")
plotOUTPE

ggsave(paste0(curWd,"Figures/outPE.tiff"), dpi=600)
```

# Individuals Differences and Model Parameters

## Correlations with Positive Learning Rate

```{r}
indDiffPlot <- posLR.corDf %>% 
  mutate(rowname = factor(rowname, levels = rowname[order(posLR)])) %>%  # Order by correlation strength
  ggplot(aes(x = rowname, y = posLR)) +
    geom_bar(stat = "identity") +
    ylab("Correlation Coefficient") +
    xlab("Individual Differences") + theme_grey(base_size = 9)  + theme(axis.text.x = element_text(angle = 90,hjust = 1)) +
  theme(axis.text.x = element_text( 
                           size = 9, angle = 45, vjust = 1)) + theme(axis.title.x = element_text(vjust=1.9)) +
  annotate("text", x = 15, y = .543, label = "*") +
  annotate("text", x = 14, y = .464, label = "*") +
  annotate("text", x = 1, y = -.525, label = "*")+
  annotate("text", x = 2, y = -.493, label = "*")+
  annotate("text", x = 3, y = -.423, label = "*") + theme(axis.text=element_text(size=9),
        axis.title=element_text(size=9,face="bold")) + theme(legend.text = element_text(size=9)) + theme(panel.border = element_rect(colour = "black", fill = NA, size =1)) + theme(legend.title = element_blank()) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))

jpeg("~/Documents/UC Riverside/Studies/Feedback Study/Manuscript/Figures/posLRindDiffp.jpg", units="in", width=6, height=4, res=300)
# insert ggplot code
indDiffPlot
dev.off()
```

### FDR Correction

```{r}
df<-AsymLR.corDf[c("posLR","RS","MAIA.All","LSA","Depr","NARQ","NARQa","NARQr","DS","SE","BF.EXT","BF.AGR","BF.CON","BF.OPE","BF.NEG", "DT.M","DT.P","BFNE","SAM","SOS")]
res2 <- Hmisc::rcorr(as.matrix(df))
pcol<-as.data.frame(res2$P)[1]
padj<-p.adjust(as.numeric(unlist(pcol)), method = "fdr", n = nrow(pcol))
PLR_pcol<-cbind(pcol, data.frame(padj))
PLR_pcol
```


## Correlations with Negative Learning Rate

```{r}
negLR.corDf %>% 
  mutate(rowname = factor(rowname, levels = rowname[order(negLR)])) %>%  # Order by correlation strength
  ggplot(aes(x = rowname, y = negLR)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with\nPositive Learning Rate") +
    xlab("Individual Differences") + theme_grey(base_size = 15)  + theme(axis.text.x = element_text(angle = 90,hjust = 1))+ theme_apa() +
  theme(axis.text.x = element_text( 
                           size = 11, angle = 45, vjust = 0.7)) + theme(axis.title.x = element_text(vjust=1.9))
```

### FDR Correction

```{r}
df<-AsymLR.corDf[c("negLR","RS","MAIA.All","LSA","Depr","NARQ","NARQa","NARQr","DS","SE","BF.EXT","BF.AGR","BF.CON","BF.OPE","BF.NEG", "DT.M","DT.P","BFNE","SAM","SOS")]
res2 <- Hmisc::rcorr(as.matrix(df))
pcol<-as.data.frame(res2$P)[1]
padj<-p.adjust(as.numeric(unlist(pcol)), method = "fdr", n = nrow(pcol))
NLR_pcol<-cbind(pcol, data.frame(padj))
NLR_pcol
```

## Correlations with Mix

```{r}
decay.corDf %>% 
  mutate(rowname = factor(rowname, levels = rowname[order(decay)])) %>%  # Order by correlation strength
  ggplot(aes(x = rowname, y = decay)) +
    geom_bar(stat = "identity") +
    ylab("Correlation with\nPositive Learning Rate") +
    xlab("Individual Differences") + theme_grey(base_size = 15)  + theme(axis.text.x = element_text(angle = 90,hjust = 1))+ theme_apa() +
  theme(axis.text.x = element_text( 
                           size = 11, angle = 45, vjust = 0.7)) + theme(axis.title.x = element_text(vjust=1.9))
```

### FDR Correction

```{r}
df<-AsymLR.corDf[c("mix","RS","MAIA.All","LSA","Depr","NARQ","NARQa","NARQr","DS","SE","BF.EXT","BF.AGR","BF.CON","BF.OPE","BF.NEG", "DT.M","DT.P","BFNE","SAM","SOS")]
res2 <- Hmisc::rcorr(as.matrix(df))
pcol<-as.data.frame(res2$P)[1]
padj<-p.adjust(as.numeric(unlist(pcol)), method = "fdr", n = nrow(pcol))
MIX_pcol<-cbind(pcol, data.frame(padj))
MIX_pcol
```

## Randomization Tests

```{r}
rand.test(AsymLR.corDf[2], AsymLR.corDf[c("RS","MAIA.All","LSA","Depr","NARQ","NARQa","NARQr","DS","SE","BF.EXT","BF.AGR","BF.CON","BF.OPE","BF.NEG", "DT.M","DT.P","BFNE","SAM","SOS")], sims = 50000, crit = 0.95, graph = TRUE, seed = 2)
```

```{r}
rand.test(AsymLR.corDf[3], AsymLR.corDf[c("RS","MAIA.All","LSA","Depr","NARQ","NARQa","NARQr","DS","SE","BF.EXT","BF.AGR","BF.CON","BF.OPE","BF.NEG", "DT.M","DT.P","BFNE","SAM","SOS")], sims = 50000, crit = 0.95, graph = TRUE, seed = 2)
```

```{r}
rand.test(AsymLR.corDf[4], AsymLR.corDf[c("RS","MAIA.All","LSA","Depr","NARQ","NARQa","NARQr","DS","SE","BF.EXT","BF.AGR","BF.CON","BF.OPE","BF.NEG", "DT.M","DT.P","BFNE","SAM","SOS")], sims = 50000, crit = 0.95, graph = TRUE, seed = 2)
```



